{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fJIu9zDXqcdw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import unittest\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0+cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Esg1dwjZqcdt",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\g'\n",
      "C:\\Users\\litvi\\AppData\\Local\\Temp\\ipykernel_12696\\3092293347.py:2: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "%run homework_modules-checkpoint.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XvLelUBpqcdy",
    "outputId": "9c9743c6-1106-4a9a-9f77-fdef49e1ffcd",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_AvgPool2d (__main__.TestLayers.test_AvgPool2d) ... ok\n",
      "test_BatchNormalization (__main__.TestLayers.test_BatchNormalization) ... ok\n",
      "test_ClassNLLCriterion (__main__.TestLayers.test_ClassNLLCriterion) ... ok\n",
      "test_ClassNLLCriterionUnstable (__main__.TestLayers.test_ClassNLLCriterionUnstable) ... ok\n",
      "test_Conv2d (__main__.TestLayers.test_Conv2d) ... ok\n",
      "test_Dropout (__main__.TestLayers.test_Dropout) ... ok\n",
      "test_ELU (__main__.TestLayers.test_ELU) ... ok\n",
      "test_Flatten (__main__.TestLayers.test_Flatten) ... ok\n",
      "test_Gelu (__main__.TestLayers.test_Gelu) ... ok\n",
      "test_GlobalAvgPool2d (__main__.TestLayers.test_GlobalAvgPool2d) ... ok\n",
      "test_GlobalMaxPool2d (__main__.TestLayers.test_GlobalMaxPool2d) ... ok\n",
      "test_LeakyReLU (__main__.TestLayers.test_LeakyReLU) ... ok\n",
      "test_Linear (__main__.TestLayers.test_Linear) ... ok\n",
      "test_LogSoftMax (__main__.TestLayers.test_LogSoftMax) ... ok\n",
      "test_MaxPool2d (__main__.TestLayers.test_MaxPool2d) ... ok\n",
      "test_Sequential (__main__.TestLayers.test_Sequential) ... ok\n",
      "test_SoftMax (__main__.TestLayers.test_SoftMax) ... ok\n",
      "test_SoftPlus (__main__.TestLayers.test_SoftPlus) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 18 tests in 57.155s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=18 errors=0 failures=0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestLayers(unittest.TestCase):\n",
    "    def test_Linear(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in, n_out = 2, 3, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Linear(n_in, n_out)\n",
    "            custom_layer = Linear(n_in, n_out)\n",
    "            custom_layer.W = torch_layer.weight.data.numpy()\n",
    "            custom_layer.b = torch_layer.bias.data.numpy()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_out)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "            # 3. check layer parameters grad\n",
    "            custom_layer.accGradParameters(layer_input, next_layer_grad)\n",
    "            weight_grad = custom_layer.gradW\n",
    "            bias_grad = custom_layer.gradb\n",
    "            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))\n",
    "            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n",
    "\n",
    "    def test_SoftMax(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Softmax(dim=1)\n",
    "            custom_layer = SoftMax()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n",
    "            next_layer_grad = next_layer_grad.clip(1e-5,1.)\n",
    "            next_layer_grad = 1. / next_layer_grad\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-5))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n",
    "\n",
    "    def test_LogSoftMax(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.LogSoftmax(dim=1)\n",
    "            custom_layer = LogSoftMax()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_BatchNormalization(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 32, 16\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            slope = np.random.uniform(0.01, 0.05)\n",
    "            alpha = 0.9\n",
    "            custom_layer = BatchNormalization(alpha)\n",
    "            custom_layer.train()\n",
    "            torch_layer = torch.nn.BatchNorm1d(n_in, eps=custom_layer.EPS, momentum=1.-alpha, affine=False)\n",
    "            custom_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n",
    "            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n",
    "\n",
    "            # 3. check moving mean\n",
    "            self.assertTrue(np.allclose(custom_layer.moving_mean, torch_layer.running_mean.numpy()))\n",
    "            # we don't check moving_variance because pytorch uses slightly different formula for it:\n",
    "            # it computes moving average for unbiased variance (i.e var*N/(N-1))\n",
    "            #self.assertTrue(np.allclose(custom_layer.moving_variance, torch_layer.running_var.numpy()))\n",
    "\n",
    "            # 4. check evaluation mode\n",
    "            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "            custom_layer.evaluate()\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            torch_layer.eval()\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "    def test_Sequential(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            alpha = 0.9\n",
    "            torch_layer = torch.nn.BatchNorm1d(n_in, eps=BatchNormalization.EPS, momentum=1.-alpha, affine=True)\n",
    "            torch_layer.bias.data = torch.from_numpy(np.random.random(n_in).astype(np.float32))\n",
    "            custom_layer = Sequential()\n",
    "            bn_layer = BatchNormalization(alpha)\n",
    "            bn_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n",
    "            bn_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "            custom_layer.add(bn_layer)\n",
    "            scaling_layer = ChannelwiseScaling(n_in)\n",
    "            scaling_layer.gamma = torch_layer.weight.data.numpy()\n",
    "            scaling_layer.beta = torch_layer.bias.data.numpy()\n",
    "            custom_layer.add(scaling_layer)\n",
    "            custom_layer.train()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-5))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-4))\n",
    "\n",
    "            # 3. check layer parameters grad\n",
    "            weight_grad, bias_grad = custom_layer.getGradParameters()[1]\n",
    "            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))\n",
    "            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n",
    "\n",
    "    def test_Dropout(self):\n",
    "        np.random.seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            p = np.random.uniform(0.3, 0.7)\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.all(np.logical_or(np.isclose(layer_output, 0),\n",
    "                                        np.isclose(layer_output*(1.-p), layer_input))))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            self.assertTrue(np.all(np.logical_or(np.isclose(layer_grad, 0),\n",
    "                                        np.isclose(layer_grad*(1.-p), next_layer_grad))))\n",
    "\n",
    "            # 3. check evaluation mode\n",
    "            layer.evaluate()\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.allclose(layer_output, layer_input))\n",
    "\n",
    "            # 4. check mask\n",
    "            p = 0.0\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.allclose(layer_output, layer_input))\n",
    "\n",
    "            p = 0.5\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "            layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            zeroed_elem_mask = np.isclose(layer_output, 0)\n",
    "            layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            self.assertTrue(np.all(zeroed_elem_mask == np.isclose(layer_grad, 0)))\n",
    "\n",
    "            # 5. dropout mask should be generated independently for every input matrix element, not for row/column\n",
    "            batch_size, n_in = 1000, 1\n",
    "            p = 0.8\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "\n",
    "            layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n",
    "\n",
    "            layer_input = layer_input.T\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n",
    "\n",
    "    def test_Conv2d(self):\n",
    "        hyperparams = [\n",
    "            {'batch_size': 8, 'in_channels': 3, 'out_channels': 6, 'height': 32, 'width': 32,\n",
    "             'kernel_size': 3, 'stride': 1, 'padding': 1, 'bias': True, 'padding_mode': 'zeros'},\n",
    "            {'batch_size': 2, 'in_channels': 3, 'out_channels': 8, 'height': 10, 'width': 10,\n",
    "             'kernel_size': 2, 'stride': (1, 2), 'padding': 0, 'bias': True, 'padding_mode': 'zeros'},\n",
    "        ]\n",
    "    \n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "    \n",
    "        for _ in range(100):\n",
    "            for params in hyperparams:\n",
    "                with self.subTest(params=params):\n",
    "                    batch_size = params['batch_size']\n",
    "                    in_channels = params['in_channels']\n",
    "                    out_channels = params['out_channels']\n",
    "                    height = params['height']\n",
    "                    width = params['width']\n",
    "                    kernel_size = params['kernel_size']\n",
    "                    stride = params['stride']\n",
    "                    padding = params['padding']\n",
    "                    bias = params['bias']\n",
    "                    padding_mode = params['padding_mode']\n",
    "    \n",
    "                    # Normalize to tuples\n",
    "                    kH, kW = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "                    sH, sW = stride if isinstance(stride, tuple) else (stride, stride)\n",
    "    \n",
    "                    # Handle 'same' padding manually\n",
    "                    if padding == 'same':\n",
    "                        pad_h = (kH - 1) // 2\n",
    "                        pad_w = (kW - 1) // 2\n",
    "                        padding = (pad_h, pad_w)\n",
    "    \n",
    "                    pad_h, pad_w = padding if isinstance(padding, tuple) else (padding, padding)\n",
    "    \n",
    "                    # Create custom layer\n",
    "                    custom_layer = Conv2d(in_channels, out_channels, (kH, kW),\n",
    "                                          stride=(sH, sW), padding=(pad_h, pad_w),\n",
    "                                          bias=bias, padding_mode=padding_mode)\n",
    "                    custom_layer.train()\n",
    "    \n",
    "                    # Create torch layer\n",
    "                    try:\n",
    "                        torch_layer = torch.nn.Conv2d(in_channels, out_channels, (kH, kW),\n",
    "                                                      stride=(sH, sW), padding=(pad_h, pad_w),\n",
    "                                                      bias=bias, padding_mode=padding_mode)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping test due to torch Conv2d error: {e}\")\n",
    "                        continue\n",
    "    \n",
    "                    # Sync weights\n",
    "                    custom_layer.weight = torch_layer.weight.detach().numpy().copy()\n",
    "                    if bias:\n",
    "                        custom_layer.bias = torch_layer.bias.detach().numpy().copy()\n",
    "    \n",
    "                    # Generate input\n",
    "                    layer_input = np.random.randn(batch_size, in_channels, height, width).astype(np.float32)\n",
    "                    input_var = torch.tensor(layer_input, requires_grad=True)\n",
    "    \n",
    "                    # Forward pass\n",
    "                    custom_output = custom_layer.updateOutput(layer_input)\n",
    "                    torch_output = torch_layer(input_var)\n",
    "                    self.assertTrue(\n",
    "                        np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-5),\n",
    "                        msg=\"Forward output mismatch\"\n",
    "                    )\n",
    "    \n",
    "                    # Backward pass\n",
    "                    next_layer_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n",
    "                    custom_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "                    torch_output.backward(torch.tensor(next_layer_grad))\n",
    "                    torch_grad = input_var.grad.detach().numpy()\n",
    "    \n",
    "                    # Check shape match\n",
    "                    self.assertEqual(torch_grad.shape, custom_grad.shape,\n",
    "                                     msg=f\"GradInput shape mismatch: {torch_grad.shape} vs {custom_grad.shape}\")\n",
    "    \n",
    "                    # Compare gradients\n",
    "                    if not np.allclose(torch_grad, custom_grad, atol=1e-4):\n",
    "                        max_diff = np.max(np.abs(torch_grad - custom_grad))\n",
    "                        print(f\"\\n⚠️ Max grad diff: {max_diff:.2e}\")\n",
    "                        print(f\"Failed params: {params}\")\n",
    "                    self.assertTrue(\n",
    "                        np.allclose(torch_grad, custom_grad, atol=1e-4),\n",
    "                        msg=\"Backward gradInput mismatch\"\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def test_LeakyReLU(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            slope = np.random.uniform(0.01, 0.05)\n",
    "            torch_layer = torch.nn.LeakyReLU(slope)\n",
    "            custom_layer = LeakyReLU(slope)\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_ELU(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            alpha = 1.0\n",
    "            torch_layer = torch.nn.ELU(alpha)\n",
    "            custom_layer = ELU(alpha)\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_SoftPlus(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Softplus()\n",
    "            custom_layer = SoftPlus()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_ClassNLLCriterionUnstable(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.NLLLoss()\n",
    "            custom_layer = ClassNLLCriterionUnstable()\n",
    "\n",
    "            layer_input = np.random.uniform(0, 1, (batch_size, n_in)).astype(np.float32)\n",
    "            layer_input /= layer_input.sum(axis=-1, keepdims=True)\n",
    "            layer_input = layer_input.clip(custom_layer.EPS, 1. - custom_layer.EPS)  # unifies input\n",
    "            target_labels = np.random.choice(n_in, batch_size)\n",
    "            target = np.zeros((batch_size, n_in), np.float32)\n",
    "            target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input, target)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(torch.log(layer_input_var),\n",
    "                                                 Variable(torch.from_numpy(target_labels).long(), requires_grad=False))\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n",
    "            torch_layer_output_var.backward()\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_ClassNLLCriterion(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.NLLLoss()\n",
    "            custom_layer = ClassNLLCriterion()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            layer_input = torch.nn.LogSoftmax(dim=1)(Variable(torch.from_numpy(layer_input))).data.numpy()\n",
    "            target_labels = np.random.choice(n_in, batch_size)\n",
    "            target = np.zeros((batch_size, n_in), np.float32)\n",
    "            target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input, target)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var,\n",
    "                                                 Variable(torch.from_numpy(target_labels).long(), requires_grad=False))\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-5))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n",
    "            torch_layer_output_var.backward()\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-4))\n",
    "\n",
    "\n",
    "    def test_MaxPool2d(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, channels, height, width = 4, 3, 16, 16\n",
    "        kernel_size, stride, padding = 2, 2, 0\n",
    "\n",
    "        for _ in range(100):\n",
    "          custom_module = MaxPool2d(kernel_size, stride, padding)\n",
    "          custom_module.train()\n",
    "\n",
    "          torch_module = torch.nn.MaxPool2d(kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "          input_np = np.random.randn(batch_size, channels, height, width).astype(np.float32)\n",
    "          input_var = torch.tensor(input_np, requires_grad=True)\n",
    "\n",
    "          custom_output = custom_module.updateOutput(input_np)\n",
    "          torch_output = torch_module(input_var)\n",
    "          self.assertTrue(\n",
    "              np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n",
    "\n",
    "          next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n",
    "          custom_grad = custom_module.updateGradInput(input_np, next_grad)\n",
    "          torch_output.backward(torch.tensor(next_grad))\n",
    "          torch_grad = input_var.grad.detach().numpy()\n",
    "          self.assertTrue(\n",
    "              np.allclose(torch_grad, custom_grad, atol=1e-5))\n",
    "\n",
    "    def test_AvgPool2d(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, channels, height, width = 4, 3, 16, 16\n",
    "        kernel_size, stride, padding = 3, 2, 1\n",
    "\n",
    "        for _ in range(100):\n",
    "          custom_module = AvgPool2d(kernel_size, stride, padding)\n",
    "          custom_module.train()\n",
    "\n",
    "          torch_module = torch.nn.AvgPool2d(kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "          input_np = np.random.randn(batch_size, channels, height, width).astype(np.float32)\n",
    "          input_var = torch.tensor(input_np, requires_grad=True)\n",
    "\n",
    "          custom_output = custom_module.updateOutput(input_np)\n",
    "          torch_output = torch_module(input_var)\n",
    "          self.assertTrue(\n",
    "              np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n",
    "\n",
    "          next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n",
    "          custom_grad = custom_module.updateGradInput(input_np, next_grad)\n",
    "          torch_output.backward(torch.tensor(next_grad))\n",
    "          torch_grad = input_var.grad.detach().numpy()\n",
    "          self.assertTrue(\n",
    "              np.allclose(torch_grad, custom_grad, atol=1e-5))\n",
    "\n",
    "    def test_Flatten(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        test_params = [\n",
    "            {'start_dim': 1, 'end_dim': -1},\n",
    "            {'start_dim': 2, 'end_dim': 3},\n",
    "            {'start_dim': 0, 'end_dim': -1},\n",
    "        ]\n",
    "\n",
    "        for _ in range(100):\n",
    "          for params in test_params:\n",
    "              with self.subTest(params=params):\n",
    "                  start_dim = params['start_dim']\n",
    "                  end_dim = params['end_dim']\n",
    "\n",
    "                  custom_module = Flatten(start_dim, end_dim)\n",
    "                  input_np = np.random.randn(2, 3, 4, 5).astype(np.float32)\n",
    "                  input_var = torch.tensor(input_np, requires_grad=True)\n",
    "\n",
    "                  custom_output = custom_module.updateOutput(input_np)\n",
    "                  torch_output = torch.flatten(input_var, start_dim=start_dim, end_dim=end_dim)\n",
    "                  self.assertTrue(\n",
    "                      np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n",
    "\n",
    "                  next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n",
    "                  custom_grad = custom_module.updateGradInput(input_np, next_grad)\n",
    "                  torch_output.backward(torch.tensor(next_grad))\n",
    "                  torch_grad = input_var.grad.detach().numpy()\n",
    "                  self.assertTrue(\n",
    "                      np.allclose(torch_grad, custom_grad, atol=1e-6))\n",
    "\n",
    "    def test_Gelu(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        for _ in range(100):\n",
    "          custom_module = Gelu()\n",
    "          custom_module.train()\n",
    "\n",
    "          torch_module = torch.nn.GELU(approximate='tanh')\n",
    "\n",
    "          input_np = np.random.randn(10, 5).astype(np.float32)\n",
    "          input_var = torch.tensor(input_np, requires_grad=True)\n",
    "\n",
    "          custom_output = custom_module.updateOutput(input_np)\n",
    "          torch_output = torch_module(input_var)\n",
    "          self.assertTrue(\n",
    "              np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-5))\n",
    "\n",
    "          next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n",
    "          custom_grad = custom_module.updateGradInput(input_np, next_grad)\n",
    "          torch_output.backward(torch.tensor(next_grad))\n",
    "          torch_grad = input_var.grad.detach().numpy()\n",
    "          self.assertTrue(\n",
    "              np.allclose(torch_grad, custom_grad, atol=1e-4))\n",
    "\n",
    "    def test_GlobalAvgPool2d(self):\n",
    "        np.random.seed(42)\n",
    "    \n",
    "        custom_layer = GlobalAvgPool2d()\n",
    "        input_tensor = np.random.rand(2, 3, 4, 4).astype(np.float32)\n",
    "        grad_output = np.ones((2, 3), dtype=np.float32)\n",
    "    \n",
    "        custom_output = custom_layer.updateOutput(input_tensor)  # ← ВАЖНО: вызов с ()\n",
    "        expected_output = np.mean(input_tensor, axis=(2, 3))\n",
    "    \n",
    "        self.assertTrue(np.allclose(custom_output, expected_output, atol=1e-6))\n",
    "    \n",
    "        custom_grad = custom_layer.updateGradInput(input_tensor, grad_output)\n",
    "        expected_grad = np.ones_like(input_tensor) * (1 / (4 * 4))\n",
    "        self.assertTrue(np.allclose(custom_grad, expected_grad, atol=1e-6))\n",
    "\n",
    "\n",
    "    def test_GlobalMaxPool2d(self):\n",
    "        np.random.seed(42)\n",
    "    \n",
    "        custom_layer = GlobalMaxPool2d()\n",
    "        input_tensor = np.random.rand(2, 3, 4, 4).astype(np.float32)\n",
    "        grad_output = np.ones((2, 3), dtype=np.float32)\n",
    "    \n",
    "        custom_output = custom_layer.updateOutput(input_tensor)\n",
    "        expected_output = np.max(input_tensor, axis=(2, 3))\n",
    "        self.assertTrue(np.allclose(custom_output, expected_output, atol=1e-6))\n",
    "    \n",
    "        custom_grad = custom_layer.updateGradInput(input_tensor, grad_output)\n",
    "        expected_grad = np.zeros_like(input_tensor)\n",
    "        mask = (input_tensor == np.max(input_tensor, axis=(2, 3), keepdims=True))\n",
    "        expected_grad[mask] = 1.0\n",
    "        self.assertTrue(np.allclose(custom_grad, expected_grad, atol=1e-6))\n",
    "\n",
    "\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestLayers)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
